\subsection*{GPU}
Once the vertices have been coloured and the code migrated to the GPU results we see are as follows.\\
\begin{figure}[H]\centering \begin{tabular}{ l | l }
  \hline
  Size & Average Over Three Time (s) \\
  \hline
  \hline
  small & 2.18 \\
  medium & 14.99 \\
  \hline
\end{tabular} \end{figure}

As we can see, moving the code to the GPU and parallelising it through colouring makes a huge difference to the performance of this algorithm. Since the GPU has 8 cores and 2 special purpose function units it can run up to 16 threads in parallel and because each vertex smoothing is performed on a single thread it can make use of this expanded parallelism. The i5 CPU on the other hand only has four cores of which two threads can be run at a time so if we had parallelised for the CPU the likelihood is the result would not have been as good. There are normally a great many more than sixteen threads that need running during a colour iteration which means that we will have many warps (32 threads); when one warp is blocked, the scheduler can schedule another to run making full use of the GPU's resources and reducing any idle time.
The trade off here is that individual thread completion times are decreased. We have increased throughput of the system, which benefits this algorithm significantly.

From profiling the small.vtk run we can gather the following statistics as a benchmark to start from:\\
\begin{figure}[H]\centering \begin{tabular}{ l | l | l | l}
\hline
Test & Average & Min & Max \\
\hline
\hline
gld\_request & 16306.99 & 418.00 & 32424.00 \\
gputime & 1402.89 & 151.14 & 2175.65 \\
occupancy & 0.22 & 0.03 & 0.25 \\
gst\_request & 169.75 & 2.00 & 384.00 \\
cputime & 1424.07 & 166.00 & 2685.00 \\
branch & 1636.28 & 214.00 & 3198.00 \\
divergent\_branch & 83.88 & 7.00 & 177.00 \\
gld\_32b & 98201.77 & 1470.00 & 171740.00 \\
gld\_coherent & 136918.82 & 1470.00 & 250401.00 \\
gld\_64b & 9956.15 & 0.00 & 19773.00 \\
instructions & 37986.54 & 5171.00 & 71584.00 \\
\hline
\end{tabular} \end{figure}

From profiling we can see that the warps have very high average divergent meaning that threads within a warp diverge frequently. This, as explained in the hypothesis, can be a major area for performance improvement and so in the next two improvements we try to focus on this.

\subsection{GPU}
An issue we encountered while migrating the code to the GPU was that the GeForce GT 330 does not provide support for double precision data. This only affects graphics cards with compute capacity 1.0 and 1.1 and so proved a difficult error to spot. Furthermore since the \verb!nvcc! compiler allowed us to successfully compile our code using doubles it meant that we saw very strange behaviour when running the algorithm and this lead us on a wild goose chase! We believe that the \verb!nvcc! compiler should not allow for compilation of code that is not supported by the hardware.
As a result we have down graded the data to floating point values, which provides no problems.

Once the vertices had been coloured and the code migrated to the GPU the results were as follows:\\

\begin{figure}[H]\centering \begin{tabular}{ l | l }
  \hline
  Size & Average Over Three Time (s) \\
  \hline
  \hline
  small & 2.18 \\
  medium & 14.99 \\
  \hline
\end{tabular} \end{figure}

As we can see, moving the code to the GPU and parallelising it through colouring makes a huge difference to the performance of this algorithm. Since the GPU has 8 cores and 2 special purpose function units it can run up to 16 threads in parallel and because each vertex smoothing is performed on a single thread it can make use of this expanded parallelism. The i5 CPU on the other hand only has four cores of which two threads can be run at a time so if we had parallelised for the CPU the likelihood is the result would not have been as good. There are normally a great many more than sixteen threads that need running during a colour iteration which means that we will have many warps (32 threads) for the scheduler to choose from, something which is a necessity when trying to achieve high throughput from a GPU. When one warp is blocked, the scheduler can schedule another to run making full use of the GPU's resources and reducing any idle time.

From profiling the \texttt{small.vtu} run we can gather the following statistics as a benchmark to start from:\\
\begin{figure}[H]\centering \begin{tabular}{ l | l | l | l}
\hline
Test & Average & Min & Max \\
\hline
\hline
gld\_request & 16306.99 & 418.00 & 32424.00 \\
gputime & 1402.89 & 151.14 & 2175.65 \\
occupancy & 0.22 & 0.03 & 0.25 \\
gst\_request & 169.75 & 2.00 & 384.00 \\
cputime & 1424.07 & 166.00 & 2685.00 \\
branch & 1636.28 & 214.00 & 3198.00 \\
divergent\_branch & 83.88 & 7.00 & 177.00 \\
gld\_32b & 98201.77 & 1470.00 & 171740.00 \\
gld\_coherent & 136918.82 & 1470.00 & 250401.00 \\
gld\_64b & 9956.15 & 0.00 & 19773.00 \\
instructions & 37986.54 & 5171.00 & 71584.00 \\
\hline
\end{tabular} \end{figure}

Here we can see that the warps have very high average divergent meaning that threads within a warp diverge frequently. This, as explained in section \ref{sec:branching}, can be a major area for performance improvement and so in the next two improvements we try to focus on this.
